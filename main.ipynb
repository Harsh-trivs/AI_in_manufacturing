{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Material</th>\n",
       "      <th>Su</th>\n",
       "      <th>Sy</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>mu</th>\n",
       "      <th>Ro</th>\n",
       "      <th>Use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANSI Steel SAE 1015 as-rolled</td>\n",
       "      <td>421</td>\n",
       "      <td>314</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7860</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANSI Steel SAE 1015 normalized</td>\n",
       "      <td>424</td>\n",
       "      <td>324</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7860</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANSI Steel SAE 1015 annealed</td>\n",
       "      <td>386</td>\n",
       "      <td>284</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7860</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANSI Steel SAE 1020 as-rolled</td>\n",
       "      <td>448</td>\n",
       "      <td>331</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7860</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANSI Steel SAE 1020 normalized</td>\n",
       "      <td>441</td>\n",
       "      <td>346</td>\n",
       "      <td>207000</td>\n",
       "      <td>79000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7860</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Material   Su   Sy       E      G   mu    Ro   Use\n",
       "0   ANSI Steel SAE 1015 as-rolled  421  314  207000  79000  0.3  7860  True\n",
       "1  ANSI Steel SAE 1015 normalized  424  324  207000  79000  0.3  7860  True\n",
       "2    ANSI Steel SAE 1015 annealed  386  284  207000  79000  0.3  7860  True\n",
       "3   ANSI Steel SAE 1020 as-rolled  448  331  207000  79000  0.3  7860  True\n",
       "4  ANSI Steel SAE 1020 normalized  441  346  207000  79000  0.3  7860  True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator,ClassifierMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv('data/data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Material    0\n",
       "Su          0\n",
       "Sy          0\n",
       "E           0\n",
       "G           0\n",
       "mu          0\n",
       "Ro          0\n",
       "Use         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before dropping duplicates : (1552, 8)\n",
      "shape after dropping duplicates (1548, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape before dropping duplicates : {data.shape}')\n",
    "data = data.drop_duplicates()\n",
    "print(f'shape after dropping duplicates {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features and target\n",
    "X = data.iloc[:, 1:-1]   # All columns except the first and last one\n",
    "y = data.iloc[:, -1]     # Only the last column as the target\n",
    "\n",
    "# Encoding target labels if needed\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X.values]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances from x to all training samples\n",
    "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train.values]\n",
    "        # Sort by distance and return indices of the first k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        # Extract the labels of the k nearest neighbor\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Return the most common class label among the k neighbors\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of custom decision tree classifier is : 99.03225806451613\n"
     ]
    }
   ],
   "source": [
    "model = CustomKNN()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(f'accuracy score of custom decision tree classifier is : {accuracy_score(y_true=y_test , y_pred= pred)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000, regularization=None, lambda_=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.regularization = regularization  # None or 'l2'\n",
    "        self.lambda_ = lambda_                # Regularization strength\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Convert X and y to NumPy arrays if they arenâ€™t already\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.iterations):\n",
    "            # Forward propagation\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Apply L2 regularization to the weight gradient\n",
    "            if self.regularization == 'l2':\n",
    "                dw += (self.lambda_ / n_samples) * self.weights\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        # Convert probabilities to 0 or 1\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return y_predicted_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression model is : 92.58\n"
     ]
    }
   ],
   "source": [
    "model = CustomLogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(f'Accuracy of logistic regression model is : {accuracy_score(y_test,pred)*100:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'iterations': 500, 'lambda_': 0.01, 'learning_rate': 0.001}\n",
      "Accuracy of logistic regression model with l2 regularisation is : 92.58\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],          # Different learning rates\n",
    "    'iterations': [500, 1000, 2000],              # Different numbers of iterations\n",
    "    'lambda_': [0.01, 0.1,0.5, 1]                     # Regularization strength for L2\n",
    "}\n",
    "\n",
    "# Initialize your custom logistic regression model\n",
    "model = CustomLogisticRegression(regularization='l2')\n",
    "\n",
    "# Set up GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Test the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Accuracy of logistic regression model with l2 regularisation is : {accuracy_score(y_test, y_pred)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value\n",
    "\n",
    "class CustomDecisionTreeClassifier():\n",
    "    def __init__(self, max_depth=2, min_samples=2):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "    \n",
    "    def create_tree(self, dataset, curr_depth=0):\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        if num_samples >= self.min_samples and curr_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                left_subtree = self.create_tree(best_split[\"dataset_left\"], curr_depth + 1)\n",
    "                right_subtree = self.create_tree(best_split[\"dataset_right\"], curr_depth + 1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_gain = -np.inf\n",
    "        \n",
    "        for feat_index in range(num_features):\n",
    "            feat_values = dataset[:, feat_index]\n",
    "            for threshold in np.unique(feat_values):\n",
    "                left, right = self.split(dataset, feat_index, threshold)\n",
    "                if(len(left) > 0 and len(right)>0):\n",
    "                    y = dataset[:, -1]\n",
    "                    y_left = left[:, -1]\n",
    "                    y_right = right[:, -1]\n",
    "                    \n",
    "                    if len(y_left) > 0 and len(y_right) > 0:\n",
    "                        curr_gain = self.calculate_gain(y, y_left, y_right)\n",
    "                        if curr_gain > max_gain:\n",
    "                            best_split[\"info_gain\"] = curr_gain\n",
    "                            best_split[\"dataset_left\"] = left\n",
    "                            best_split[\"dataset_right\"] = right\n",
    "                            best_split[\"threshold\"] = threshold\n",
    "                            best_split[\"feature_index\"] = feat_index\n",
    "                            max_gain = curr_gain\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def calculate_gain(self, y, y_left, y_right):\n",
    "        total = len(y)\n",
    "        total_l = len(y_left)\n",
    "        total_r = len(y_right)\n",
    "        \n",
    "        gini = 1 - sum([(np.sum(y == c) / total) ** 2 for c in np.unique(y)])\n",
    "        gini_l = 1 - sum([(np.sum(y_left == c) / total_l) ** 2 for c in np.unique(y_left)])\n",
    "        gini_r = 1 - sum([(np.sum(y_right == c) / total_r) ** 2 for c in np.unique(y_right)])\n",
    "\n",
    "        gain = gini - ((total_l / total) * gini_l + (total_r / total) * gini_r)\n",
    "        return gain\n",
    "    \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        # Convert the target values to integers before applying bincount\n",
    "        Y = Y.astype(int)\n",
    "        return np.bincount(Y).argmax()\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = y.ravel()\n",
    "        y = y.reshape(-1,1)\n",
    "        dataset = np.concatenate((X, y), axis=1)\n",
    "        self.root = self.create_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            predictions.append(self.make_prediction(X.iloc[i,:],self.root))\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "        \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of custom decision tree classifier is : 100.0\n"
     ]
    }
   ],
   "source": [
    "model = CustomDecisionTreeClassifier(max_depth=10)\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(f'accuracy score of custom decision tree classifier is : {accuracy_score(y_true=y_test , y_pred= pred)*100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
